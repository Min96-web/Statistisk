---
title: "Statistik och dataanalys I, 15 hp "
subtitle: "Inl칛mningsuppgift 1"
author: 
- Johanna Sel칬
- Minhui Zhong
date: last-modified
format: 
  html:
    self-contained: true
  pdf: default  
  docx: default
toc: true
language: 
  title-block-author-single: " "
toc-title-document: "Inneh친ll"
crossref-fig-title: "Figur"
theme: lumen
title-block-banner-color: Primary
title-block-published: "Publicerad"
callout-warning-caption: "Varning"
callout-note-caption: "Observera"
callout-tip-caption: "Tips"
editor: visual
---

::: callout-warning
Den h칛r inl칛mningsuppgiften f칬ruts칛tter att f칬ljande paket finns installerade:

-   `mosaic`

-   `dplyr`

-   `geosphere`

-   `leaflet`

Paket kan installeras via kommandot `install.packages('packagename')`, d칛r `'packagename'` 칛r namnet p친 paketet, t.ex `'mosaic'`.
:::

## Introduktion

I den f칬rsta inl칛mningsuppgiften ska ni sj칛lvst칛ndigt i grupper om tre analysera ett dataset i programmeringsspr친ket R. Till skillnad fr친n datorlaborationerna finns det minimalt med kodexempel. Datorlaborationerna g친r igenom de flesta momenten som behandlas i inl칛mningsuppgiften, s친 se till att g칬ra klart dessa innan.

------------------------------------------------------------------------

::: callout-note
### Instruktioner

I denna inl칛mningsuppgift ska ni analysera ett datamaterial som beskriver ca 500 distrikt i Boston 친r 1970. Datasetet 칛r en modifierad version[^1] av originaldata[^2] som anv칛ndes i en studie[^3] d칛r f칬rfattarna predikterade medianhuspriser i olika distrikt med hj칛lp av en rad f칬rklaringsvariabler.

F칬ljande variabler finns i datasetet `boston_census_data.Rdata` ([ladda ner](https://github.com/StatisticsSU/SDA1/blob/main/assignments/assignment1/Boston_census_data.RData?raw=true)), som inneh친ller 480 observationer. Notera att en observation motsvarar ett distrikt:

-   `town`: Stadsdel.
-   `longitude`: Longitud koordinat.
-   `latitude`: Latitud koordinat.
-   `median_home_value`: Medianhuspriset (enhet 1K USD).
-   `crime_rate`: Brott (per 1000 inv친nare).
-   `zoned_25k_p`: Andel av stadsdelens bostadsmark 칛mnad f칬r marklotter st칬rre 칛n 25000 kvadratfot.
-   `indust_p`: Andel tunnland 칛gd av f칬retag utanf칬r detaljhandel.
-   `borders_charles`: Charles River dummy variabel (= 1 om omr친det gr칛nsar till floden, 0 annars).
-   `NOx`: Koncentration av kv칛veoxider (andelar per 10 miljon).
-   `n_rooms_avg`: Genomsnitt antal rum i 칛gda bost칛der.
-   `before_1940_p`: Andel 칛gda bost칛der byggda f칬re 1940.
-   `employ_dist`: Viktat avst친nd till fem arbetsf칬rmedlingscentra i Boston.
-   `radial_access`: Index som m칛ter tillg친ng till stadsmotorv칛gar.
-   `tax_rate`: Fastighetsskatt per 10000 USD.
-   `pupil_teacher_ratio`: L칛rart칛thet m칛tt som elev per l칛rare.
-   `lower_stat_pct`: Procentandel med l친g socioekonomisk status i termer av utbildning eller arbete.
-   `dist_fenway_park`: Avst친nd till stadion Fenway Park.

Inl칛mningsuppgiften ska l칛mnas in i form av ett html-dokument genererat av Quarto. **Kontrollera att ni inte f친r n친gra felmeddelande n칛r du genererar HTML-dokumentet. L칛s sedan igenom HTML-dokumentet noggrant innan ni l칛mnar in det.** Anv칛nd tydliga figurer och namnge axlarna med tydliga variabelnamn. Gl칬m inte att skriva era namn i b칬rjan av dokumentet, d칛r det nu st친r Namn 1, Namn 2 och Namn 3.
:::

[^1]: Totalunders칬kningen trunkerade medianhusv칛rdet till 50K f칬r de censusdistrikten som l친g 칬ver. Vi har tagit bort dessa censusdistrikt. Vi har ocks친 tagit bort variabler som 칛r irrelevanta.

[^2]: Harrison Jr, D., & Rubinfeld, D. L. (1978). Hedonic housing prices and the demand for clean air. Journal of Environmental Economics and Management, 5(1), 81-102.

[^3]: Pace, R. K., & Gilley, O. W. (1997). Using the spatial configuration of the data to improve estimation. The Journal of Real Estate Finance and Economics, 14(3), 333-340.

## 0. Ladda in data

#### 游눩 Uppgift 0.1

Ladda in datasetet `Boston_census_data` med f칬ljande kod.

::: {.callout-note appearance="minimal"}
# Uppgift 0.1 - Svar

```{r}
# Write your code here
load(file = url("https://github.com/StatisticsSU/SDA1/blob/main/assignments/assignment1/Boston_census_data.RData?raw=true")) 
```
:::

## 1. Kriminalitet i Boston

I detta avsnitt ska ni analysera kriminaliteten i Boston med hj칛lp av variabeln `crime_rate`.

#### 游눩 Uppgift 1.1

Vad kan man generellt s칛ga om kriminaliteten i Boston 1970? Anv칛nd l칛mpliga figurer och m친tt f칬r att ge en beskrivning.

::: {.callout-note appearance="minimal"}
# Uppgift 1.1 - Svar

Variabeln crime_rate m칛ter brott per 1000 inv친nare. Vi b칬rjar med att anv칛nda kommandot "favstats" f칬r att f친 en bra 칬verblick 칬ver variabeln. Favstats s칛ger oss att det minsta v칛rdet 칛r 0.00632 och 88.9762 칛r det h칬gsta, det ger en varians p친 88,96988 vilket i sammanhanget 칛r v칛ldigt mycket och s칛ger oss att brottsligheten skiljer sig v칛ldigt mycket 친t mellan olika distrikt. Medelv칛rdet 칛r 3.66526 med en standardavvikelse p친 8.746 medan Medianen enbart 칛r 0.253715 med en IQR p친 3.599. I och med att datasetet 칛r snedvridet s친 anser vi att medianen 칛r ett b칛ttre m친tt f칬r denna variabel.

Genom att anv칛nda en boxplot bekr칛ftar det tanken p친 att datasetet 칛r v칛ldigt snedvridet d친 vi ser v칛ldigt m친nga outliers. Vi kan 칛ven se om vi g칬r ett histogram ist칛llet att majoriteten av distrikten, 칬ver 80%, har en crime_rate p친 mellan 0 och 10 brott per tusen inv친nare, vilket med andra ord betyder att de flesta distrikten har en relativt l친g crime_rate medan n친gra f친 distrikt har en v칛ldigt h칬g crime_rate.

```{r}
# Write your code here
suppressMessages(library(mosaic))
suppressMessages(library(dplyr))
suppressMessages(library(geosphere))
suppressMessages(library(leaflet))
favstats(~crime_rate, data = Boston_census_data)
bwplot(~crime_rate, data=Boston_census_data)
histogram(~crime_rate, data = Boston_census_data, breaks = 10)

```
:::

#### 游눩 Uppgift 1.2

Distrikten tillh칬r olika stadsdelar som anges i den kategoriska variabeln `town`? Det finns 88 olika s친dana stadsdelar (towns).

*Skiljer sig brottsligheten 친t mellan de olika stadsdelarna?* Unders칬k stadsdelarna `Boston East Boston`, `Boston Downtown`, `Cambridge`, samt ytterligare en stadsdel som ni sj칛lva v칛ljer. Besvara fr친gan med hj칛lp av l칛mpligt valda figurer och statistiska m친tt.

::: callout-tip
Innan ni p친b칬rjar analysen, skapa en ny data frame som enbart inneh친ller de stadsdelar som ni vill j칛mf칬ra. Det kan g칬ras till exempel med funktionen `filter()`.
:::

::: {.callout-note appearance="minimal"}
# Uppgift 1.2 - Svar

Efter att vi har skapat v친rt filter som enbart inneh친ller de utvalda stadsdelarna, n=63 ist칛llet f칬r 480, kan precis som p친 f칬rra deluppgiften b칬rja med kommandot favstats. D칛r ser vi att variansen fortfarande 칛r v칛ldigt stor 51.1358 - 0.29819 = 50.83761 samt att medianen och medelv칛rdet skiljer sig 친t, median = 2.3139 och medelv칛rdet = 6.261704. Det s칛ger oss att f칬rdelningen fortfarande 칛r skev, b친de medelv칛rdet och medianen 칛r h칬gre h칛r 칛n om vi tittar p친 alla observationer.

Tittar vi p친 histogrammet ser vi tydligt att den 칛r h칬gerskev med en markant outlier med en crime_rate p친 50 per 1000 inv친nare. F칬rdelningen 칛r unimodal med ett typv칛rde p친 en crime_rate mellan 0 och 10 per 1000 inv친nare.

Avslutningsvis g칬r vi en korstabell, d칛r vi ser att om man st칛ller stadsdelarna mot varandra, att majoriteten av brotten sker I Cambridge (41,27%).

```{r}
# Write your code here
Boston_census_new <- filter(Boston_census_data, town == "Boston East Boston" |town == "Boston Downtown" |town == "Cambridge" |town == "Newton")
favstats(~crime_rate, data = Boston_census_new)
histogram(~crime_rate, data = Boston_census_new)
tally(~town + crime_rate, data = Boston_census_new, format = "percent", margins = TRUE)
```
:::

#### 游눩 Uppgift 1.3

Vilka tv친 variabler i datasetet `Boston_census_data` korrelerar mest med brottslighet? Beskriv sambandet mellan brottslighet och var och en av dessa tv친 variabler.

::: callout-tip
Kom ih친g att korrelation m칛ter det linj칛ra sambandet mellan *numeriska variabler*.
:::

::: {.callout-note appearance="minimal"}
# Uppgift 1.3 - Svar

Vi skapar ett nytt dataset som enbart inneh친ller numeriska variabler som vi d칬per till "Boston_census_data_15_variables". Sen f칬r att hitta vilka tv친 variabler som korrelerar mest med brottslighet g칬r vi en correlation matrix, denna 칛r dock sv친r칬versiktlig s친 d친 칛r det b칛ttre att g칬ra en corrplot, d칛r man tydligare ser vad som korrelerar med vad. Detta kan man senare dubbelkolla med correlation matrixen, ifall vissa pluppar 칛r snarlika varandra.

Vi hittar 3 variabler som ser ut att korrelera mest med crime_rate: radial_access, median_home_value och tax_rate. Vi anv칛nder funktionen cor() f칬r att se vilka tv친 som har h칬gst samband. radial_access: 0.62, median_home_value: -0.45 och tax_rate: 0.58.

Radial_access och crime_rate har ett relativt starkt positivt samband p친 0.62, det inneb칛r ju st칬rre tillg친ng till stadsmotorv칛gar desto h칬gre crime_rate.

tax_rate och crime_rate har ocks친 ett postivt samband p친 0.58, det inneb칛r att ju h칬gre tax_rate desto mer brott beg친s per 1000 inv친nare.

```{r}
# Write your code here
Boston_census_data_15_variables <- Boston_census_data[, c("longitude","latitude","median_home_value","crime_rate","zoned_25k_p","indust_p","NOx","n_rooms_avg","before_1940_p","employ_dist","radial_access","tax_rate","pupil_teacher_ratio","lower_stat_pct","dist_fenway_park")]
correlation_matrix_Boston <- cor(Boston_census_data_15_variables)
suppressMessages(library(corrplot))
corrplot(correlation_matrix_Boston)
cor(radial_access ~ crime_rate, data = Boston_census_data)
cor(median_home_value ~ crime_rate, data = Boston_census_data)
cor(tax_rate ~ crime_rate, data = Boston_census_data)

```
:::

## 2. Fastighetsskatt i Boston

I detta avsnitt ska ni analysera fastighetsskatten i Boston med hj칛lp av variabeln `tax_rate`.

#### 游눩 Uppgift 2.1

Vad kan man generellt s칛ga om fastighetsskatten i distrikten? Anv칛nd l칛mpliga figurer och m친tt f칬r att beskriva f칬rdelningen.

::: {.callout-note appearance="minimal"}
# Uppgift 2.1 - Svar

Fastighetsskatten varierar mellan 187 och 711 per 1000 USD, vilket ger en varians p친 524. Medianen och Medelv칛rdet skiljer sig inte lika mycket som det gjorde p친 brottsligheten, utan ligger h칛r p친 330 respektive 409.33. Standardavvikelsen 칛r 168.5777 och IQR 칛r 385.

Om vi g칬r ett histogram ser vi att det 칛r en bimodal f칬rdelning samt att det ser ut att vara tv친 olika grupper i och med att det 칛r ett glapp mellan staplarna. Typv칛rderna ligger runt 300 och 650. Antagligen finns det ett samband med en annan variabel som avg칬r om man betalar l칛gre eller h칬gre fastighetsskatt.

```{r}
# Write your code here
favstats(~tax_rate, data=Boston_census_data)
histogram(~ tax_rate, data = Boston_census_data, col = "navyblue", type="count")
```
:::

#### 游눩 Uppgift 2.2

L친t oss skapa en ny variabel `cat_tax` som anger om ett distrikt betalar l친g (`low`), medel (`medium`), eller h칬g (`high`) fastighetsskatt. Vi definerar skattekategorierna enligt

-   `low`: `tax_rate` $\leq$ 250,
-   `medium`: 250 $<$ `tax_rate` $\leq$ 400,
-   `high`: `tax_rate` $>$ 400.

F칬ljande kod skapar och l칛gger till variabeln `cat_tax` i `Boston_census_data`

```{r}
Boston_census_data$cat_tax <- cut(Boston_census_data$tax_rate, 
              breaks=c(0, 250, 400, 800),
              labels=c('Low', 'Medium', 'High'))
```

Finns det ett samband mellan vilken skattekategori ett distrikt tillh칬r och dess angr칛nsning till Charles River? F칬rklara med hj칛lp av l칛mplig tabell och figur.

::: {.callout-note appearance="minimal"}
# Uppgift 2.2 - Svar

Som man ser i stapeldiagrammet skiljer sig skatteniv친erna inte s친 mycket 친t mellan om man gr칛nsar till Charles River eller inte. Det man kan se 칛r att det 칛r mer troligt att man har men medium-tax rate om man gr칛nsar till Charles River 칛n om man inte g칬r det.

F칬rdelningen mellan skatteniv친er om man inte gr칛nsar till Charles River eller om man g칬r det kan vi se i korstabellen. Det 칛r st칬rre sannolikhet att man inte gr칛nsar till Charles River om man har l친g (13,3% mot 10,3%) eller h칬g (39,69% mot 34,48%) skattekategori.

```{r}
# Write your code here
tally(cat_tax ~ borders_charles, data = Boston_census_data, margins=TRUE, format="percent")
bargraph(~cat_tax, groups=borders_charles, data=Boston_census_data, type="percent", main = "Skattekategorier baserat p친 angr칛sning till Charles River")
```
:::

#### 游눩 Uppgift 2.3

Hur m친nga procent av alla distrikt i v친r data ligger i angr칛nsning till Charles River *och* tillh칬r en h칬g skattekategori? Hur stor andel av distrikten med h칬g skatt ligger *inte* i angr칛nsning till Charles River?

::: {.callout-note appearance="minimal"}
# Uppgift 2.3 - Svar

Nedan ser vi f칬rst en marginalf칬rdelningstabell. D칛r kan vi se att 2,08% av alla distrikt i v친r data angr칛nsar till River Charles (1) och tillh칬r en h칬g skattekategori (High).

Vi ser sedan en betingad f칬rdelningstabell, som 칛r uppdelad p친 skattekategorierna. I den kan vi se att av de distrikt som har en h칬g skattekategori 칛r det 94,71% som inte ligger i angr칛nsning till River Charles (0).

```{r}
# Write your code here
tally(~ cat_tax + borders_charles, data=Boston_census_data, format="percent", margins=T)
tally(borders_charles ~ cat_tax, data = Boston_census_data, margins=TRUE, format="percent")
```
:::

#### 游눩 Uppgift 2.4

Vilka tv친 variabler i datasetet `Boston_census_data` korrelerar starkast med `tax_rate`? Beskriv det parvisa sambandet mellan `tax_rate` och var och en av dessa tv친 variabler. Vad kan vi s칛ga om kausalitet f칬r vart och ett av sambanden?

::: callout-tip
Kom ih친g att korrelation 칛r ett m친tt p친 linj칛ra samband mellan *numeriska variabler*.
:::

::: {.callout-note appearance="minimal"}
# Uppgift 2.4 - Svar

Skriv svaret h칛r.

Vi skapar ett nytt dataset som enbart inneh친ller numeriska variabler som vi d칬per till "Boston_census_data_15_variables". Sen f칬r att hitta vilka tv친 variabler som korrelerar mest med fastighetsskatt g칬r vi en correlation matrix, denna 칛r dock sv친r칬versiktlig s친 d친 칛r det b칛ttre att g칬ra en corrplot, d칛r man tydligare ser vad som korrelerar med vad. Detta kan man senare dubbelkolla med correlation matrixen, ifall vissa pluppar 칛r snarlika varandra.

Man ser ganska tydligt att det 칛r tv친 variabler som verkar korrelera mest med tax_rate och det 칛r radial_access och indust_p. Vi anv칛nder cor()-funktionen f칬r att se korrelationerna.

Radial_access och tax_rate har ett v칛ldigt starkt positivt samband p친 0.91. Det verkar finnas ett tydligt samband mellan st칬rre tillg친ng till stadsmotorv칛gar och distriktens fastighetsskatt.

츿ven indust_p och tax_rate har ett starkt positivt samband p친 0.72. indust_p st친r f칬r andel tunnland 칛gd av f칬retag utanf칬r detaljhandeln. Det kan man tolka att ju mer tunnland som 칛gs av f칬retag utanf칬r detaljhandeln, desto h칬gre tax_rate.

Vi kan inte s칛ga att n친gon av dessa 칛r kausala samband, vi vet inte med s칛kerhet att det 칛r t ex tillg친ngen till stadsmotorv칛gar som p친verkar fastighetsskatten, utan det kan alltid finnas en dold variabel utanf칬r v친rt dataset som p친verkar b친de, och bara f친r det att se ut som att det finns ett samband.

```{r}
# Write your code here
Boston_census_data_15_variables <- Boston_census_data[, c("longitude","latitude","median_home_value","crime_rate","zoned_25k_p","indust_p","NOx","n_rooms_avg","before_1940_p","employ_dist","radial_access","tax_rate","pupil_teacher_ratio","lower_stat_pct","dist_fenway_park")]
suppressMessages(library(corrplot))
corrplot(correlation_matrix_Boston)
cor(radial_access ~ tax_rate, data = Boston_census_data)
cor(indust_p ~ tax_rate, data = Boston_census_data)
```
:::

## 3. Avst친nd till Fenway park

I detta avsnitt ska ni unders칬ka variabeln `dist_fenway_park`, som m칛ter avst친ndet mellan ett distrikt och Fenway park (stadion d칛r basebollslaget Boston Red Sox spelar sina hemmamatcher).

Vi kan visualisera Fenway park och distrikten p친 en karta med hj칛lp av R-paketet `leaflet`. F칬ljande kod visar platsen f칬r Fenway park och distrikten f칬r observationerna 30 och 45.

```{r}
#| eval: false
library(leaflet) # Install if not available
fenway_park_lat_long <- c(42.346462, -71.097250) # latitude and longitude for Fenway_park
Boston_map <- leaflet() %>% 
  addTiles() %>%
  addMarkers(lat = fenway_park_lat_long[1], lng = fenway_park_lat_long[2], popup="Fenway park") %>%
  addMarkers(lat = Boston_census_data$latitude[30], lng = Boston_census_data$longitude[30], popup="Observation 30") %>%
  addMarkers(lat = Boston_census_data$latitude[45], lng = Boston_census_data$longitude[45], popup="Observation 45") 

Boston_map # Show interactive map
```

#### 游눩 Uppgift 3.1

Vilket distrikt i v친r data har l칛ngst respektive kortast avst친nd till Fenway park? Markera ut dessa distrikt i en interaktiv karta tillsammans med Fenway park.

::: {.callout-note appearance="minimal"}
# Uppgift 3.1 - Svar

Det f칬rsta vi m친ste g칬ra 칛r att f친 fram vilka distrikt som har l칛ngst respektive kortast avst친nd till Fenway park. Det f친r vi fram genom att sortera datan p친 "dist_fenway_park"-variablen. Om vi anv칛nder funktionen head() kan vi se distrikten p친 f칬rst raden. Wilmington har kortast avst친nd p친 887.9 medan Marshfield har l칛ngst p친 33638.4.

F칬r att f친 ut dessa p친 kartan, betyder vi ut till v친ra nya variabler f칬r den sorterade datan och v칛ljer den f칬rsta raden i respektive.

```{r}
#| eval: false
#enklaste metod
which.min(Boston_census_data$dist_fenway_park)
which.max(Boston_census_data$dist_fenway_park)
Boston_map <- leaflet() %>% 
  addTiles() %>%
  addMarkers(lat = fenway_park_lat_long[1], lng = fenway_park_lat_long[2], popup="Fenway park") %>%
  addMarkers(lat = Boston_census_data$latitude[143], lng = Boston_census_data$longitude[143], popup="Observation 143") %>%
  addMarkers(lat = Boston_census_data$latitude[25], lng = Boston_census_data$longitude[25], popup="Observation 45") 
Boston_map

dist_fenway_sorted <- Boston_census_data %>% arrange(dist_fenway_park)
dist_fenway_sorted_hightolow <- Boston_census_data %>% arrange(desc(dist_fenway_park))

library(leaflet) 
fenway_park_lat_long <- c(42.346462, -71.097250) # latitude and longitude for Fenway_park
Boston_map <- leaflet() %>% 
  addTiles() %>%
  addMarkers(lat = fenway_park_lat_long[1], lng = fenway_park_lat_long[2], popup="Fenway park") %>%
  addMarkers(lat = dist_fenway_sorted$latitude[1], lng = dist_fenway_sorted$longitude[1], popup="kortast") %>%
  addMarkers(lat = dist_fenway_sorted_hightolow$latitude[1], lng = dist_fenway_sorted_hightolow$longitude[1], popup="Observation l칛ngst") 
Boston_map
```
:::

#### 游눩 Uppgift 3.2

Finns det ett samband mellan `dist_fenway_park` och `crime_rate`?

::: {.callout-note appearance="minimal"}
# Uppgift 3.2 - Svar

Det finns ett svagt negativt samband mellan avst친ndet till Fenway park och crime_rate. Korrelationen antyder att n칛rmare Fenway park (l칛gre avst친nd) desto mer brottslighet (h칬gre crime_rate). Dock 칛r sambandet svagt s친 det kan likav칛l bero p친 slumpen.

```{r}
# Write your code here
cor(dist_fenway_park ~ crime_rate, data = Boston_census_data)
```
:::

## 4. Enkel linj칛r regression

I detta avsnitt ska ni anpassa och tolka enkla linj칛ra regressionsmodeller.

#### 游눩 Uppgift 4.1

Anpassa en linj칛r regression med responsvariabeln `NOx` och den f칬rklarande variabeln `employ_dist`. Rita den anpassade regressionslinjen tillsammans med data i en l칛mplig figur. Beskriv resultaten och tolka modellen. Utf칬r en modellvalidering via en residualanalys och kommentera modellens l칛mplighet. Om modellen inte anses l칛mplig, vilka antaganden har inte varit uppfyllda?

::: {.callout-note appearance="minimal"}
# Uppgift 4.1 - Svar

Vi tolkar 58.61% av variationen i responsvariabeln NOx f칬rklaras av variabeln employ_dist. Minsta kvadratanpassningen 칛r

$$\widehat{NOx} = b_0 + b_1employdist = 0.718 - 0.043employdist.$$

Tolkningen f칬r b0 = 0.718 칛r den predikterade genomsnittliga NOx och employ_dist 칛r 0, vilket inte 칛r meningfullt. Vi kan inte g칬ra en kausal tolkning f칬r b1 eftersom det inte 칛r s친 att employ_dist medf칬r b칛ttre eller s칛mre NOx. Vi kan s칛ga att employ_dist som 칛r 1 tenderar att i genomsnitt minskar 0.043 fler enheter NOx. V친r anpassade modell ger oss 480 prediktioner av de genomsnittliga NOx, dvs en prediktion $\hat{y}_i$ (NOx) f칬r varje $x_i$ (employ_dist) i datasetet.

```{r}
lm_NOx_vs_employdist <- lm(NOx ~ employ_dist, data = Boston_census_data)
summary(lm_NOx_vs_employdist)

plot(NOx ~ employ_dist, data = Boston_census_data, col = "cornflowerblue", ylim = c(0, 1))
y_hat <- predict(lm_NOx_vs_employdist)
head(y_hat)
lines(Boston_census_data$employ_dist, y_hat, type = "p", col = "lightcoral")
#plotta data tillsammans med de predikterade v칛rden i samma figur med hj칛lp av funktionen lines().
abline(lm_NOx_vs_employdist, col = "lightcoral") 
#funktionen abline() som ritar den r칛ta linjen (minsta kvadratanpassningen).
legend(x = "topleft", pch = c(1, 1, NA), lty = c(NA, NA, 1), col = c("cornflowerblue", "lightcoral", "lightcoral"), legend=c("Data", "Predicted", "Fitted line"))

# residualanalys
Boston_census_data$res <- resid(lm_NOx_vs_employdist)
Boston_census_data$y_hatt <- fitted(lm_NOx_vs_employdist)
plot(Boston_census_data$res ~ Boston_census_data$y_hatt, ylab="Resid", xlab="y-hatt", main="Residplot")
abline(h=0)

# residualer
resid <- residuals(lm_NOx_vs_employdist)
head(resid)
qqnorm(resid, col = "cornflowerblue") # Create normal probability plot for residuals
qqline(resid, col = "red") # Add a straight line to normal probability plot 

# Den 칛r inte s친 slumpm칛ssig, inte normalf칬rdelad.
```
:::

#### 游눩 Uppgift 4.2

Anv칛nd modellen i Uppgift 4.1 f칬r att prediktera koncentration av kv칛veoxider f칬r observation 10, d칛r `employ_dist`=10.5857. Ber칛kna vad residualen blir f칬r denna observation.

::: {.callout-note appearance="minimal"}
# Uppgift 4.2 - Svar

Skriv svaret h칛r.

```{r}
# Ber칛kna residualen
new_x <- data.frame(employ_dist = c(10.5857))
predict(lm_NOx_vs_employdist, newdata = new_x)

```
:::

#### 游눩 Uppgift 4.3

Transformera variablerna i Uppgift 4.1 (avg칬r sj칛lv vilken eller vilka av de tv친 som beh칬ver transformeras). Ett f칬rslag 칛r att anv칛nda Tukeys cirkel f칬r att hitta l칛mpliga transformationer. Anpassa en ny linj칛r regression med de transformerade variablerna. Utf칬r en modellvalidering (efter transformation) via en residualanalys och kommentera modellens l칛mplighet j칛mf칬rt med modellen i Uppgift 4.1.

::: {.callout-note appearance="minimal"}
# Uppgift 4.3 - Svar

Skriv svaret h칛r.

```{r}
# Write your code here

plot(NOx ~ employ_dist, data = Boston_census_data,col = "cornflowerblue") #otransformerad

# Testar olika transformationer
plot(sqrt(NOx) ~ sqrt(employ_dist), data = Boston_census_data, col = "cornflowerblue")
plot(sqrt(NOx) ~ employ_dist, data = Boston_census_data, col = "cornflowerblue")
plot(NOx ~ sqrt(employ_dist), data = Boston_census_data, col = "cornflowerblue")
plot(log(NOx) ~ log(employ_dist), data = Boston_census_data, col = "pink")
plot(NOx ~ log(employ_dist), data = Boston_census_data, col = "pink")
plot(log(NOx) ~ employ_dist, data = Boston_census_data, col = "pink")
plot(sqrt(NOx) ~ log(employ_dist), data = Boston_census_data, col = "coral")
plot(log(NOx) ~ sqrt(employ_dist), data = Boston_census_data, col = "coral")

# Vi v칛ljer att transformera b친de x och y till log(x) och log(y) f칬r det ger den r칛taste linjen.
lm_logNOx_vs_logemploy_dist <- lm(log(NOx) ~ log(employ_dist), data = Boston_census_data)
summary(lm_logNOx_vs_logemploy_dist)
plot(log(NOx) ~ log(employ_dist), data = Boston_census_data, col = "cornflowerblue")
log_y_hat <- predict(lm_logNOx_vs_logemploy_dist)

orginal_skala_y <- exp(log_y_hat) #transformera tillbaka orginaldata y

plot(log(NOx) ~ log(employ_dist), data = Boston_census_data, col = "cornflowerblue")
lines(Boston_census_data$employ_dist, log_y_hat, type ="p", col = "lightcoral")
abline(lm_logNOx_vs_logemploy_dist, col = "lightcoral")
legend(x = "topleft", pch = c(0,5, 0,5, NA), lty = c(NA, NA, 0,5), col = c("cornflowerblue", "lightcoral", "lightcoral"), legend=c("Data", "Predicted", "Fitted line"))

# residualanalys
Boston_census_data$res <- resid(lm_logNOx_vs_logemploy_dist)
Boston_census_data$y_hatt <- fitted(lm_logNOx_vs_logemploy_dist)
plot(Boston_census_data$res ~ Boston_census_data$y_hatt, ylab="Resid", xlab="y-hatt", main="Residplot")
abline(h=0)

# residualer
resid <- residuals(lm_logNOx_vs_logemploy_dist)
head(resid)

qqnorm(resid, col = "cornflowerblue") # Create normal probability plot for residuals
qqline(resid, col = "red") # Add a straight line to normal probability plot 

```
:::

#### 游눩 Uppgift 4.4

Plotta den anpassade regressionen fr친n 4.3 i icke-transformerad skala tillsammans med observationerna (ocks친 i icke-transformerad skala) i en l칛mplig figur.

::: {.callout-note appearance="minimal"}
# Uppgift 4.4 - Svar

Skriv svaret h칛r.

```{r}
# Write your code here

logy_hat <- predict(lm_logNOx_vs_logemploy_dist) # log scale prediction
y_hat <- exp(logy_hat) # original scale prediction
head(y_hat)

plot(NOx ~ employ_dist, data = Boston_census_data, col = "cornflowerblue") # Data on original scale
lines(Boston_census_data$employ_dist, y_hat, type = "p", col = "lightcoral")
legend(x = "topleft", pch = c(1, 1), col = c("cornflowerblue", "lightcoral"), legend=c("Data", "Predicted"))
```
:::

#### 游눩 Uppgift 4.5

Anv칛nd modellen i Uppgift 4.3 f칬r att prediktera koncentration av kv칛veoxider i icke-transformerad skala f칬r observation 10, d칛r `employ_dist`=10.5857. Ber칛kna vad residualen blir f칬r denna observation. Kommentera resultaten j칛mf칬rt med Uppgift 4.2.

::: callout-tip
T칛nk p친 att ta h칛nsyn till eventuella transformationer!
:::

::: {.callout-note appearance="minimal"}
# Uppgift 4.5 - Svar

Skriv svaret h칛r.

```{r}
# Write your code here
new_x <- data.frame(employ_dist = c(10.5857))
log_y_hatt=predict(lm_logNOx_vs_logemploy_dist, newdata = new_x)
y_hatt=exp(log_y_hatt)
y=Boston_census_data$NOx[10]

# Ber칛kna residualen
resid <- y-y_hatt 
#residualen 칛r ca 0.042
```
:::

## 5. Multipel linj칛r regression

I detta avsnitt ska ni studera multipel linj칛ra regression.

#### 游눩 Uppgift 5.1

Anpassa en linj칛r regression med responsvariabel logaritmerad `median_home_value` samt f칬rklarande variabler `lower_stat_pct` och dummy-variabeln `borders_charles`. Tolka koefficienten f칬r `borders_charles`.

::: {.callout-note appearance="minimal"}
# Uppgift 5.1 - Svar

Fr친n utskriften kan vi utl칛sa att $b_0 = 32.367$, $b_1 = -0.844$ och $b_2=2.37$. Vidare kan vi utl칛sa att ca 59% av variationen i median_home_value f칬rklaras av lower_stat_pct och borders_charles. I en multipel regression s친 tolkar vi alltid de skattade effektena av en variabel givet att alla andra variablerna h친lls konstanta. Till exempel, givet att f칬rklarande variabler lowe_stat_pct 칛r samma, s친 tenderar summy_variabeln borders_charles i genomsnitt har $b_2=2.37$ fler enheter 칬kat i den skattade responsvariabel logaritmerad median_home_value. Givet att summy_variabeln borders_charles 칛r samma, s친 tenderar f칬rklarande variabler lowe_stat_pct i genomsnitt har $b_1=-0.844$ fler enheter minskat i den skattade responsvariabel logaritmerad median_home_value.

```{r}
lm_median_home_value_vs_lower_stat_pct_borders_charles <- lm(log(median_home_value) ~ lower_stat_pct + borders_charles, data = Boston_census_data)
summary(lm_median_home_value_vs_lower_stat_pct_borders_charles)

logy_hat <- predict(lm_median_home_value_vs_lower_stat_pct_borders_charles)# log scale prediction

lmod_boston <- lm(median_home_value ~ lower_stat_pct + borders_charles, data=Boston_census_data)
lmod_boston

```
:::

#### 游눩 Uppgift 5.2

Ni ska nu utforma en modell som predikterar medianhuspriset `median_home_value`. Ni f친r endast anv칛nda f칬ljande f칬rklaringsvariabler:

-   `before_1940_p`
-   `crime_rate`
-   `radial_access`
-   `NOx`
-   `dist_fenway_park`

Ni f친r sj칛lva v칛lja hur m친nga av variablerna som ska ing친 i modellen. Ni f친r g칬ra vilka transformationer ni vill av variablerna, inklusive responsvariabeln.

Pr칬va er fram metodiskt n칛r ni v칛ljer vilka variabler ni inkluderar i modellen, och n칛r ni best칛mmer vilka eventuella transformationer ni anv칛nder.

N칛r ni utv칛rderar olika modeller kan ni f칬rslagsvis b칬rja med att j칛mf칬ra adjusted R-squared. N칛r ni med hj칛lp av adjusted R-squared har identifierat tv친 eller tre modeller som ser lovande ut kan ni utv칛rdera dessa modeller ytterligare i ett andra steg.

I det andra steget ska ni utv칛rdera hur v칛l modellerna predikterar data som *inte* anv칛nts f칬r att anpassa modellen. Ni kan v칛lja *en av tv친 alternativa metoder*:

1.  Dela in ert dataset i tr칛ningsdata och testdata. Anpassa modellen med hj칛lp av tr칛ningsdata, och utv칛rdera sedan p친 testdata. Ni kan exempelvis anv칛nda de f칬rsta 350 observationerna som tr칛ningsdata och de sista 130 observationerna som testdata.
2.  Anv칛nd korsvalidering. Det 칛r en n친got mer kr칛vande metod, men ocks친 n친got b칛ttre. Ni kan exempelvis g칬ra korsvalidering med 4 folds (4-fold cross validation). Dela d친 upp ert dataset i fyra delar (del 1: observationer 1-120, del 2: observationer 121-240, del 3: observationer 241-360, del 4: observationer 361-480).

*Sortera inte observationerna i `Boston_census_data` slumpm칛ssigt. Ordningen 칛r redan slumpm칛ssig.*

::: callout-tip
T칛nk p친 att ta h칛nsyn till eventuell transformation av responsvariabeln. Om ni exempelvis har valt transformationen $\log(y)$ 칛r modellens prediktion av responsvariabeln $\widehat{\log(y)}$. Ni m친ste d친 transformera den till $\hat y$ i responsvariabelns originalskala med formeln $\hat{y}=\exp\left(\widehat{\log(y)}\right)$. Sedan kan ni r칛kna ut residualen $y - \hat y$.
:::

::: {.callout-note appearance="minimal"}
# Uppgift 5.2 - Svar

Vi har tr칛nat och testat b친de model 1 och model 2 med f칬rsta method att dela 350 som tr칛ningdata och resten 130 칛r testdata, model 1 har h칬gre RMSE 칛n model 2 d칛rf칬r 칛r model 2 b칛ttre.

```{r}
#1 Dela dataset i 350 observationer som tr칛ningsdata och 130 observationer som testdata
library(dplyr)
data_randomorder <- Boston_census_data %>% slice_sample(prop=1)
datatrain <- Boston_census_data %>% slice(1:350) # De f칬rsta 350 observationerna
datatest <- Boston_census_data %>% slice(351:480) # De sista 130 observationerna

#Anv칛nd en linj칛r regression som modell
model1 <- lm(log(median_home_value) ~ before_1940_p, data=datatrain)
model2 <- lm(log(median_home_value) ~ before_1940_p + NOx, data=datatrain) #Tr칛na modellen 2

#Tr칛na model 1
y_hatt_1 <- predict(model1, newdata=datatest)
y_hatt <- exp(y_hatt_1)
y_test <- datatest$median_home_value
SSE <- sum((y_test - y_hatt)^2)
n_train <- 130
MSE <- (SSE / n_train)
RMSE <- sqrt(MSE)
RMSE
#Tr칛na model 2
y_hatt_2 <- predict(model2, newdata=datatest)
y_hatt_test <- exp(y_hatt_test_2)
y_test <- datatest$median_home_value
SSE <- sum((y_test - y_hatt_test)^2)
n_train <- 130
MSE <- (SSE / n_train)
RMSE <- sqrt(MSE)
RMSE

#2 Korsvalidering f칬r Modellen 
n <- 480 # Number of observations
# Fold 1:
obs_index <- c(1:n) # Keeps track of the indices of  the dataset (1, 2, 3, ...., n = 480)
test_fold_index <- obs_index[c(1:120)] # Subsets indices 1:120 (test data fold 1) 
training_fold_index <- obs_index[-c(1:120)] # Takes out the complement
lm_modell1_fold1 <- lm(median_home_value ~ before_1940_p, subset = training_fold_index, data = Boston_census_data) # Estimate fold 1
test_data <- Boston_census_data[test_fold_index, ] # Create test data for fold
y_hat_fold1 <- predict(lm_modell1_fold1, newdata = test_data) # Predict test data in scale
SSE_fold1 <- sum((test_data$median_home_value - y_hat_fold1)^2) 

# Fold 2:
test_fold_index <- obs_index[c(121:240)] # Subsets indices 121:240 (test data fold 2) 
training_fold_index <- obs_index[-c(121:240)] # Takes out the complement
lm_modell1_fold2 <- lm(median_home_value ~ before_1940_p, subset = training_fold_index, data = Boston_census_data) # Estimate fold 2
test_data <- Boston_census_data[test_fold_index, ] # Create test data for fold
y_hat_fold2 <- predict(lm_modell1_fold2, newdata = test_data) # Predict test data in scale
SSE_fold2 <- sum((test_data$median_home_value - y_hat_fold2)^2)
# Fold 3:
test_fold_index <- obs_index[c(241:360)] # Subsets indices 241:360 (test data fold 3) 
training_fold_index <- obs_index[-c(241:360)] # Takes out the complement
lm_modell1_fold3 <- lm(median_home_value ~ before_1940_p, subset = training_fold_index, data = Boston_census_data) # Estimate fold 3
test_data <- Boston_census_data[test_fold_index, ] # Create test data for fold
y_hat_fold3 <- predict(lm_modell1_fold3, newdata = test_data) # Predict test data in scale
SSE_fold3 <- sum((test_data$median_home_value - y_hat_fold3)^2)
# Fold 4:
test_fold_index <- obs_index[c(361:480)] # Subsets indices 361:480 (test data fold 4) 
training_fold_index <- obs_index[-c(361:480)] # Takes out the complement
lm_modell1_fold4 <- lm(median_home_value ~ before_1940_p, subset = training_fold_index, data = Boston_census_data) # Estimate fold 4
test_data <- Boston_census_data[test_fold_index, ] # Create test data for fold
y_hat_fold4 <- predict(lm_modell1_fold4, newdata = test_data) # Predict test data in scale
SSE_fold4 <- sum((test_data$median_home_value - y_hat_fold4)^2)

n <- nrow(Boston_census_data)
RMSE = sqrt((SSE_fold1 + SSE_fold2 +SSE_fold3 +SSE_fold4)/n)
RMSE

# Modell 2 fick vi ca 6.71 som 칛r mindre modell 1 som ligger ca 6.84 d칛rf칬r modell 2 칛r b칛ttre.

obs_index <- c(1:n) # Keeps track of the indices of  the dataset (1, 2, 3, ...., n = 480)
test_fold_index <- obs_index[c(1:120)] # Subsets indices 1:120 (test data fold 1) 
training_fold_index <- obs_index[-c(1:120)] # Takes out the complement
lm_modell1_fold1 <- lm(median_home_value ~ before_1940_p + NOx, subset = training_fold_index, data = Boston_census_data) # Estimate fold 1
test_data_2 <- Boston_census_data[test_fold_index, ] # Create test data for fold
y_hat_fold1_test <- predict(lm_modell2_fold1, newdata = test_data) # Predict test data in scale
SSE_fold1_test <- sum((test_data_2$median_home_value - y_hat_fold1_test)^2) 

# Fold 2:
test_fold_index <- obs_index[c(121:240)] # Subsets indices 121:240 (test data fold 2) 
training_fold_index <- obs_index[-c(121:240)] # Takes out the complement
lm_modell2_fold2 <- lm(median_home_value ~ before_1940_p + NOx, subset = training_fold_index, data = Boston_census_data) # Estimate fold 2
test_data_2 <- Boston_census_data[test_fold_index, ] # Create test data for fold
y_hat_fold2_test <- predict(lm_modell2_fold2, newdata = test_data) # Predict test data in scale
SSE_fold2_test <- sum((test_data_2$median_home_value - y_hat_fold2_test)^2)

# Fold 3:
test_fold_index <- obs_index[c(241:360)] # Subsets indices 241:360 (test data fold 3) 
training_fold_index <- obs_index[-c(241:360)] # Takes out the complement
lm_modell2_fold3 <- lm(median_home_value ~ before_1940_p + NOx, subset = training_fold_index, data = Boston_census_data) # Estimate fold 3
test_data_2 <- Boston_census_data[test_fold_index, ] # Create test data for fold
y_hat_fold3_test <- predict(lm_modell2_fold3, newdata = test_data) # Predict test data in scale
SSE_fold3_test <- sum((test_data_2$median_home_value - y_hat_fold3_test)^2)

# Fold 4:
test_fold_index <- obs_index[c(361:480)] # Subsets indices 361:480 (test data fold 4) 
training_fold_index <- obs_index[-c(361:480)] # Takes out the complement
lm_modell2_fold4 <- lm(median_home_value ~ before_1940_p + NOx, subset = training_fold_index, data = Boston_census_data) # Estimate fold 4
test_data_2 <- Boston_census_data[test_fold_index, ] # Create test data for fold
y_hat_fold4_test <- predict(lm_modell2_fold4, newdata = test_data) # Predict test data in scale
SSE_fold4_test <- sum((test_data_2$median_home_value - y_hat_fold4_test)^2)

n <- nrow(Boston_census_data)
RMSE = sqrt((SSE_fold1_test + SSE_fold2_test +SSE_fold3_test +SSE_fold4_test)/n)
RMSE

```
:::

#### 游눩 Uppgift 5.3

G칬r en residualanalys av den valda modellen i Uppgift 5.2.

::: {.callout-note appearance="minimal"}
# Uppgift 5.3 - Svar

```{r}
#Anv칛nd en linj칛r regression som modell
model_1 <- lm(log(median_home_value) ~ before_1940_p, data=datatrain) #Tr칛na modellen 1
model_2 <- lm(log(median_home_value) ~ before_1940_p + NOx, data=datatest) #Tr칛na modellen 2

# residualer
resid_model_1 <- residuals(model_1)
head(resid_model_1)
plot(datatrain$median_home_value, resid_model_1, xlab= "median_home_value", ylab='Residuals', col = "cornflowerblue") 
qqnorm(resid_model_1, col = "cornflowerblue") # Create normal probability plot for residuals
qqline(resid_model_1, col = "red") # Add a straight line to normal plot

resid_model_2 <- residuals(model_2)
head(resid_model_2)
plot(datatest$median_home_value, resid_model_2, xlab= "median_home_value", ylab='Residuals', col = "cornflowerblue") 
qqnorm(resid_model_2, col = "cornflowerblue") # Create normal probability plot for residuals
qqline(resid_model_2, col = "red") # Add a straight line to normal plot 
```
:::

#### 游눩 Uppgift 5.4

Anv칛nd modellen i Uppgift 5.2 f칬r att prediktera medianhuspriset f칬r observationerna i datasetet `Boston_districts_to_predict` ([ladda ner](https://github.com/StatisticsSU/SDA1/blob/main/assignments/assignment1/Boston_districts_to_predict.RData?raw=true)).

::: {.callout-note appearance="minimal"}
# Uppgift 5.4 - Ladda hem data f칬r prediktion

Ladda in dataseten `Boston_districts_to_predict` med f칬ljande kod.

```{r}
# Write your code here
load(file = url("https://github.com/StatisticsSU/SDA1/blob/main/assignments/assignment1/Boston_districts_to_predict.RData?raw=true")) 
```
:::

Det h칛r datasetet har endast de f칬rklarande variablerna, dvs inte responsvariabeln. N칛r vi r칛ttar era inl칛mningsuppgifter kommer vi att j칛mf칬ra era prediktioner med de faktiska medianpriserna (som vi har tillg친ng till).

Skriv ut dina prediktioner s친 att vi enkelt kan se dem n칛r vi r칛ttar.

::: callout-tip
T칛nk p친 att ta h칛nsyn till eventuella transformationer av f칬rklaringsvariablerna!
:::

::: {.callout-note appearance="minimal"}
# Uppgift 5.4 - Svar

Skriv svaret h칛r.

```{r}
new_x <- data.frame(Boston_districts_to_predict)
predict(model_2, newdata = new_x)
```
:::
